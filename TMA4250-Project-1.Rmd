---
title: "TMA4250-Project-1"
author: "Ole Riddervold, Ole Kristian Skogly"
date: "2023-02-07"
output: html_document
---
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)

<<<<<<< HEAD
=======
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)

>>>>>>> OKS
# install.packages("geoR")
# install.packages("akima")
# install.packages("fields")
# install.packages("ggplot2")
# install.packages("MASS")
<<<<<<< HEAD
# install.packages("reshape2")
# install.packages("patchwork")
# install.packages("glue")
# install.packages("tidyverse")
# install.packages("latex2exp")
=======
>>>>>>> OKS

library(geoR)
library(akima)
library(fields)
library(ggplot2)
library(MASS)
<<<<<<< HEAD
library(reshape2)
library(patchwork)
library(glue)
library(tidyverse)
library(latex2exp)
=======
library(bessel)
library(ggplot2)
library(plotrix)
library(viridis)
>>>>>>> OKS
```

# Problem 1

## a)
<<<<<<< HEAD
Definition of positive semi-definite:
$\forall m \geq 1$, $\forall a_1, \cdots, a_m \in \mathbb{R}$ and $\forall \mathbf{s}_1, \cdots, \mathbf{s}_m \in \mathcal{D}$:
$$
\begin{align}
  \sum_{i=1}^m{\sum_{j=1}^m}{a_ia_jc(\mathbf{s_i}, \mathbf{s_j})}
\end{align}
$$
The requirement for the correlation function to be positive semi-definite is necessary because it ensures that the covariance matrix formed from the correlation function is positive semi-definite. The covariance matrix is an important aspect of a stationary Gaussian random field, as it captures the covariance structure of the random field, which is essential for many statistical models that make use of the it. A positive semi-definite covariance matrix ensures that the covariance between any two points in the field is always non-negative, and that the that the covariance matrix can always be factored into a lower triangular matrix multiplied by its transpose, which is useful for computation and interpretation.

The powered exponential correlation function is given by
$$
\rho(h) = \exp{\left(-\frac{h}{a}\right)^\alpha}
$$
The Matern with smoothness $\nu$ is given by
$$
\rho(h) = \frac{2^{1-\nu}}{\Gamma(\nu)}\left( \frac{\sqrt{8\nu}h}{a} \right)^\nu K_\nu\left(\frac{\sqrt{8\nu}h}{a}  \right)
$$
The semi-variogram function is defined as
$$
\gamma(h) = \frac{1}{2}\mathrm{Var}\left[ X(h)-X(0) \right]
$$
Which in both cases is
$$
\gamma(h)= \sigma^2(1-\rho(h))
$$

```{r 1a1}
h <- seq(0, 50, by=0.1)

powered_exponential <- function(h, a, alpha) {
  # Powered exponential correlation function as defined in lectures
  return(
    exp(-(h/a)^alpha)
  )
}

matern_modified <- function(h, a, nu) {
  # Matérn correlation function as defined in lectures (based on Matérn from geoR)
  phi <- a/sqrt(8*nu)
  return(
    matern(h, phi, nu)
  )
}
```

Plotting:
```{r 1a2}
# Plot the functions
plot(h, powered_exponential(h, a=10, alpha=1), type="l", lwd=2, col="red", xlab="h", ylab="ρ(h)")+title("Spatial correlation functions")
lines(h, powered_exponential(h, a=10, alpha=1.9), lwd=2, col="blue")
lines(h, matern_modified(h, a=20, nu=1), type="l", lwd=2, col="green")
lines(h, matern_modified(h, a=20, nu=3), type="l", lwd=2, col="purple")

legend("topright", c("Power Exponential (α=1, a=10)", "Power Exponential (α=1.9, a=10)", "matern (ν=1, a=10)", "matern (ν=3, a=10)"),
       col=c("red", "blue", "green", "purple"), lwd=2, bg="white")

#Variance= 1
var1 = 1
for (var in c(1, 3)) {
  #Semi-variogram function for the Power Exponential with power alpha=1 and spatial scale a=10
  semivariogram_funcexp1 = var*(1-powered_exponential(h, a=10, alpha=1))
  #Semi-variogram function for the Power Exponential with power alpha=1.9 and spatial scale a=10
  semivariogram_funcexp2 = var*(1-powered_exponential(h, a=10, alpha=1.9))
  #Semi-variogram function for #matern with smoothness nu=1
  semivariogram_corr_funcmatern1 = var*(1-matern_modified(h, a=20, nu=1))
  #Semi-variogram function for #matern with smoothness nu=3
  semivariogram_corr_funcmatern2 = var*(1-matern_modified(h, a=20, nu=3))
  
  # Plot the semivariograms for varians= 1
  plot(h, semivariogram_funcexp1, type="l", lwd=2, col="red", xlab="h", ylab="γ(h)")+title(TeX(glue("Semi-variogram for $\\sigma^2={var}$")))
  lines(h, semivariogram_funcexp2, lwd=2, col="blue")
  lines(h, semivariogram_corr_funcmatern1, type="l", lwd=2, col="green")
  lines(h, semivariogram_corr_funcmatern2, type="l", lwd=2, col="purple")
  
  legend("bottomright", c("Power Exponential (α=1, a=10)", "Power Exponential (α=1.9, a=10)", "matern (ν=1, a=10)", "matern (ν=3, a=10)"),
         col=c("red", "blue", "green", "purple"), lwd=2, bg="white")

}
```



## b)
$X$ is a GRF, and therefore, by definition,
$$
\mathbf{X} \sim \mathcal{N}(\mathbf{\mu}, \Sigma)
$$
where $\mu_i = m(s_i)$ and $\Sigma_{ij} = c(s_i, s_j)$. In this particular case, we get $\mathbf{\mu} = \mathbf{0}$ and $\Sigma_{ij} = \sigma^2 \rho(|s_i-s_j|)$, where $\rho$ is the *correlation function*, in this case of type either powered exponential or Matérn.

The following code yields a simulation with four realizations for the values $\sigma^2 \in \{1, 5\}$, $\alpha \in \{1, 1.9\}$, $\nu \in \{1, 3\}$:

Defining parameters that are constant for the different cases:
```{r 1b1}
s <- c(1:50)
N <- 4
variance <- c(1, 5)
array_dims <- c(4, 4, length(s)) # Dimensions of the data-array to be plotted

dist_matrix <- as.matrix(dist(s, diag=TRUE, upper=TRUE))
mu <- rep(0, length(s))
```

Defining a function to plot the resulting 3D-array:
```{r 1b2_plot_fnc}
plot_realizations <- function(realizations, titles) {
  
  plot <- do.call(wrap_plots, lapply(1:4, function(i) { # Using patchwork to create subplots using `wrap_plots`
    df <- realizations %>% melt %>% filter(Var1 == i)   # Converts the 3D array into a dataframe and filters on i
    ggplot(df) + geom_line(aes(Var3, value, group=Var2, color=factor(Var2))) +
      xlab("s") + labs(title=titles[i], color="")
  }))
  
  return(plot)
}
```


Loop for powered exponential correlation function:
```{r 1b2_sim}
set.seed(42)

alpha <- c(1, 1.9)
exp_a <- 10
exp_realizations <- array(rep(0, prod(array_dims)), array_dims)

for (i in seq_along(variance)) {
  for (j in seq_along(alpha)) {
    Sigma <- variance[i]*powered_exponential(dist_matrix, exp_a, alpha[j])
    flattened_index <- 2*(i-1)%%2 + j
    exp_realizations[flattened_index, , ] <- as.array(mvrnorm(n=N, mu, Sigma))
  }
}
```


```{r 1b2_plot}
titles <- c(TeX("$\\sigma^2 = 1, \\alpha = 1.0$"), TeX("$\\sigma^2 = 1, \\alpha = 1.9$"),
            TeX("$\\sigma^2 = 5, \\alpha = 1.0$"), TeX("$\\sigma^2 = 5, \\alpha = 1.9$"))
plot_realizations(exp_realizations, titles)
```


Loop for Matérn correlation functions:
```{r 1b3_sim}
set.seed(42)

nu_values <- c(1, 3)
matern_a <- 10
matern_realizations <- array(rep(0, prod(array_dims)), array_dims)

for (i in seq_along(variance)) {
  for (j in seq_along(nu_values)) {
    Sigma <- variance[i]*matern(dist_matrix, matern_a, nu_values[j])
    flattened_index <- 2*(i-1)%%2 + j
    matern_realizations[flattened_index, , ] <- as.array(mvrnorm(n=N, mu, Sigma))
  }
}
```

```{r 1b3_plot}
titles <- c(TeX("$\\sigma^2 = 1, \\nu = 1$"), TeX("$\\sigma^2 = 1, \\nu = 3$"),
            TeX("$\\sigma^2 = 5, \\nu = 1$"), TeX("$\\sigma^2 = 5, \\nu = 3$"))
plot_realizations(matern_realizations, titles)
```

## c)

We have $\mathbf{Y} = \mathbf{X}^* + \mathbf{\varepsilon}$, with $\mathbf{X}^* = \left(X(s_1), \: X(s_2), \: X(s_3)\right)^T$, $\mathbf{X}^*$ is distributed as above, but only for the first three entries of $\tilde{\mathcal{D}}$, and $\mathbf{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2_NI)$. Since $\mathbf{X}^*$ and $\mathbf{\varepsilon}$ are uncorrelated, we get
$$
\mathbf{Y} \sim \mathcal{N}(\mathbf{0}, \bar{\Sigma} + \sigma^2_NI)
$$
where $\bar{\Sigma} = \mathrm{Var}(\mathbf{X}^*)$.

## d)

In this problem, consider
$$
\begin{pmatrix}
  \mathbf{X} \\
  \mathbf{Y}
\end{pmatrix}
\sim
\mathcal{N}\left( \mathbf{0},
  \begin{pmatrix}
    \Sigma_{00} & \Sigma_{01} \\
    \Sigma_{10} & \Sigma_{11}
  \end{pmatrix}
\right)
$$
where $\Sigma_{00} = \Sigma$, $\Sigma_{11} = \bar{\Sigma} + \sigma_N^2I$, and
$$
\begin{align}
  \Sigma_{10} = \Sigma_{01}^T &= \mathrm{Cov}(\mathbf{X}, \mathbf{Y}) = \mathrm{Cov}(\mathbf{X}, \mathbf{X}^*+\mathbf{\varepsilon}) \\
  &= \Sigma^* + \mathrm{Cov}(\mathbf{X}, \mathbf{\varepsilon}) = \Sigma^* \equiv \mathrm{Cov}(\mathbf{X}^*, \mathbf{X})
\end{align}
$$

Thus, looking at a formula for basic conditional expectation for multivariate gaussian distributions, we get:
$$
\mathbf{X} | \mathbf{Y}=\mathbf{y} \sim \mathcal{N}\left( \Sigma^{*T}\left(\bar{\Sigma}+\sigma_N^2I\right)^{-1}\mathbf{y}, \Sigma - \Sigma^{*T}\left(\bar{\Sigma}+\sigma_N^2I\right)^{-1} \Sigma^* \right)
$$
Simulation:

```{r 1d1}
set.seed(42)

Sigma <- variance[2]*matern(dist_matrix, matern_a, nu_values[2])
sigma_N <- c(0, 0.25)
Sigma_star <- Sigma[1:3, ]
Sigma_bar <- Sigma_star[, 1:3]

# Assuming we use the same y throughout problem 1
y <- as.vector(mvrnorm(n=1, rep(0, 3), Sigma_bar + 0.25*identity(3)))

array_dims <- c(50, 4)
cond_sim <- array(rep(0, prod(array_dims)), array_dims)
for (i in seq_along(sigma_N)) {
  inv_mat <- solve(Sigma_bar + sigma_N[i]*identity(3))
  cond_mu <-  t(Sigma_star) %*% inv_mat %*% y
  cond_Sigma <- Sigma - t(Sigma_star) %*% inv_mat %*% Sigma_star
  cond_sim[, i] <- mvrnorm(n=1, cond_mu, cond_Sigma)
  cond_sim[, i+2] <- diag(cond_Sigma) # 95% CI
}
```

Creating a data frame:
```{r 1d2}
z <- 1.64 # z-value for a confidence interval of level 0.10

cond_sim <- as.data.frame(cond_sim)
colnames(cond_sim) <- c("Rel1", "Rel2", "Rel1_95", "Rel2_95")
cond_sim$s <- s
cond_sim$Rel1_95 <- sqrt(cond_sim$Rel1_95)*z
cond_sim$Rel2_95 <- sqrt(cond_sim$Rel2_95)*z
head(cond_sim)
```


Plotting:
```{r 1d3}
p1 <- ggplot(cond_sim) +
  geom_line(aes(s, Rel1)) +
  geom_ribbon(aes(x=s, ymin=Rel1-Rel1_95, ymax=Rel1+Rel1_95), alpha=0.25) +
  ylab("Realization 1") + labs(title=TeX(glue("$\\sigma_N^2 = {sigma_N[1]}$")))

p2 <- ggplot(cond_sim) +
  geom_line(aes(s, Rel2)) + 
  geom_ribbon(aes(x=s, ymin=Rel2-Rel2_95, ymax=Rel2+Rel2_95), alpha=0.25) +
  ylab("Realization 2") + labs(title=TeX(glue("$\\sigma_N^2 = {sigma_N[2]}$")))

p1 + p2
```

The nugget variance is clearly apparent in the second plot, and the mean is also slightly different based on this. Furthermore, the predicted mean differs slightly at the observed points due to the presence of $\sigma_N^2$ in the inverted matrix.

## e)

100 realizations for the two nugget variances.
```{r 1e1}
estimate_cond_realization <- function(sigma_N, n, seed=42) {
  # Given some nugget variance, simulates conditional distribution of X
  # on the given grid with observation y = (y_1, y_2, y_3).
  set.seed(seed)
  inv_mat <- solve(Sigma_bar + sigma_N*identity(3))
  cond_mu <-  t(Sigma_star) %*% inv_mat %*% y
  cond_Sigma <- Sigma - t(Sigma_star) %*% inv_mat %*% Sigma_star
  return(mvrnorm(n=n, cond_mu, cond_Sigma))
}

array_dims <- c(2, 50, 100)
cond_sim <- array(rep(0, prod(array_dims)), array_dims)
for (i in seq_along(sigma_N)) {
  cond_sim[i, ,] <- estimate_cond_realization(sigma_N[i], n=100)
}
```
Note: This also holds for any other choice of locations, but using the first three makes indexing easier.

Empirical estimation of the distribution.
```{r 1e2}
melted_cond_sim <- cond_sim %>%
  melt() %>%
  group_by(Var1, Var2) %>%
  mutate(mean_est=mean(value), PI90_est=1.64*sqrt(1+var(value))/10) %>%
  nest(-Var1)

head(melted_cond_sim$data[1])
```
Plotting:
```{r 1e3}
p1 <- as.data.frame(melted_cond_sim$data[1]) %>% ggplot() +
  geom_line(aes(x=Var2, y=value, group=Var3, color=factor(Var3))) +
  geom_line(aes(x=Var2, y=mean_est)) +
  geom_ribbon(aes(x=Var2, ymin=mean_est-PI90_est, ymax=mean_est+PI90_est), alpha=0.25) +
  xlab("s") + ylab("Relizations") +
  labs(title=TeX(glue("$\\sigma_N^2 = {sigma_N[1]}$"))) +
  theme(legend.position="none")

p2 <- as.data.frame(melted_cond_sim$data[2]) %>% ggplot() +
  geom_line(aes(x=Var2, y=value, group=Var3, color=factor(Var3))) +
  geom_line(aes(x=Var2, y=mean_est)) +
  geom_ribbon(aes(x=Var2, ymin=mean_est-PI90_est, ymax=mean_est+PI90_est), alpha=0.25) +
  xlab("s") + ylab("Relizations") +
  labs(title=TeX(glue("$\\sigma_N^2 = {sigma_N[2]}$"))) +
  theme(legend.position="none")

p1 + p2
```
The noise from each realization makes it very difficult to see a pattern in this realization, apart from that they all share the estimated mean and variance-diagonal in general. It is difficult to compare these realizations with the single prediction from task d).

## f)
Regular MC integral estimate of $\mathbb{E}(A)$:
```{r 1f1}
sim <- cond_sim[1, , ]
sim_stats <- sim %>%
  melt() %>%
  group_by(Var2) %>%
  summarise(A_hat=sum((value>2)*(value-2))) %>%
  summarise(A_bar=mean(A_hat), sigma_hat=sd(A_hat))

sim_stats
```
One can use simple kriging to calculate predictions at all values of $s$ based on our observation $\mathbf{y}$. The formula is as follows:
$$
\begin{align}
  \hat{\mathbf{X}} &= \mathbf{a}^T\mathbf{Y} \\
  \mathbf{a} &= \Sigma_{\mathbf{Y}}\mathrm{Cov}(\mathbf{Y}, \mathbf{X}) 
\end{align}
$$
where both $\Sigma_{\mathbf{Y}}\in \mathbb{R}^{3\times3}$ and $\mathrm{Cov}(\mathbf{Y}, \mathbf{X}) \in \mathbb{R}^{50\times 3}$ can be retrieved from $\Sigma$.

Simple 1D Kriging using the Matérn covariance function:
```{r 1f2}
set.seed(10000)
y <- as.vector(mvrnorm(n=1, rep(0, 3), Sigma_bar)) # Draw of observed y with sigma_N = 0

c_matrix <- Sigma[, 1:3] # Represents Cov(Y, X_0) for all X_0 in D
a_matrix <- c_matrix %*% t(solve(Sigma_bar)) # Calculation of all a for the BLUP
X_hat_kriging <- a_matrix %*% y
```

```{r 1f3}
krig_stats <- X_hat_kriging %>%
  melt() %>%
  summarise(A_tilde=sum((value>2)*(value-2)))

krig_stats
```

Firstly, $\hat{X}(s)$ is an unbiased predictor, so $\mathbb{E}(\hat{X}(s))=X(s)$. Furthermore, Jensen's inequality states that for a concave function $g: \mathbb{R} \rightarrow \mathbb{R}$
$$
g\left(\mathbb{E}(X)\right) \leq \mathbb{E}\left(g(X)\right)
$$
In this case, $g: \mathcal{L}^2 \rightarrow \mathcal{L}^2$ is given by
$$
g(x) = \sum_{s\in\tilde{\mathcal{D}}}{\mathbb{I}(x(s)>2)(x(s)-2)}
$$
where $\mathcal{L}^2$ is the Hilbert space of random variables (in this case a random field indexed by $s$). $g$ is convex, but not strictly convex, i.e.
$$
\begin{align}
  \tilde{A}=g\left(\mathbb{E}(X)\right) &\leq \mathbb{E}\left(g(X)\right)=\hat{A} \\
  \implies \tilde{A} &\leq \hat{A}
\end{align}
$$
This is very clearly apparent in the simulations above, in which the Kriging estimate suspiciously never generated samples above 2 (it is probable that this is due to some error in the calculations.)

## g)
For the exponential and the Matérn covariance functions, the plots in task b clearly present a larger "long-scale" variance with changing $\sigma^2$, and a larger smoothness with changing hyperparameters $\alpha$ and $\nu$ respectively. A conditional model is in both cases relatively easy to achieve using basic multivariate Gaussian conditional distributions, as was presented in d) and e). Finally, we studied two different methods for estimating parameters of the distribution, one empirical and one using the BLUP (best linear unbiased predictor), a.k.a. simple Kriging. Unfortunately, the latter method did not yield interesting results.

# Problem 2

## a)

## b)

## c)

## d)

=======
## b)

## c)

## d)

## e)

## f)

## g)


# Problem 2

## a)
```{r 1b1}
#Read the data from topo.dat file
data <- read.table("/Users/olekristianskogly/Documents/Romlig stat/topo.dat", sep = " ", header = TRUE)
ggplot (data, aes(x, y, color = z)) + geom_point()

# Interpolate the data
interp_obj <- interp(data$x, data$y, data$z)

# Plot the interpolated data with color bar
image.plot(interp_obj, main = "Interpolated Elevation", xlab = "x", ylab = "y")



interp_obj <- interp(data$x, data$y, data$z)

# Plot the heatmap
image(interp_obj, xlab = "x", ylab = "y", col = colorRampPalette(c("blue", "white", "red"))(30), 
      useRaster = TRUE)

# Add a contour plot on top of the heatmap
contour(interp_obj, add = TRUE, drawlabels = TRUE)
```

## b)
For the universal kriging predictor we need to find the best linear unbiased predictor(BLUP) of the underlying process at location $\vec{s_{0}} \in D$. We know from the lecture notes that when determine the BLUP for $\mathbf{X_{0}} = \mathbf{X}({s_{0}})$ at an unobserved location $\vec{s_{0}} \in D$, we need to determine $\vec{a} \in 	\mathbb{R}^{n}$ such that $\hat{\mathbf{X_{0}}} = \vec{a}^{T} \vec{\mathbf{X}}$ satisfies: 

\begin{enumerate}

\item $E[\hat{\mathbf{X_{0}}}] = E[\mathbf{X_{0}}]$.

 \item MSE = $E[(\mathbf{X_{0}} - \hat{\mathbf{X_{0}}})^{2}]$ is minimized. 
 
\end{enumerate}

Starting by showing that the predictor is unbiased for an arbitrary location $\vec{s_{0}} \in D$: 

\begin{equation}
    E[\hat{\mathbf{X_{0}}}] = E[a^{T} \mathbf{X}] = a^{T} E[\mathbf{X}] = a^{T} \cdot g(s)^{T} \beta
\end{equation}

\begin{equation}
    E[\mathbf{X_{0}}] = g(s_{0})^{T} \beta
\end{equation}

when setting $E[\hat{\mathbf{X_{0}}}] = E[\mathbf{X_{0}}]$, we get $a^{T} g(s)^{T} = g(s_{0})^{T}$ since $\beta$ disappear from each side of the equation. Further we want to calculate the weights such that it satisfies the property for having a BLUP:




\begin{multline}
\centering
    \hat{\alpha} &=\arg \max_{a}   (E[(\mathbf{X_{0}} - \hat{\mathbf{X_{0}}})^{2}]) &=\\
     \arg \max_{a} \mathrm{Var}(\mathbf{X_{0}} - \hat{\mathbf{X_{0}}} ) &=\\
     \arg \max_{a} (\mathrm{Cov}[\hat{\mathbf{X_{0}}} - \mathbf{X_{0}}, \hat{\mathbf{X_{0}}} - \mathbf{X_{0}} ]) &=\\
    \arg \max_{a} (\mathrm{Cov}[a^{T}\mathbf{X} - \mathbf{X_{0}}, a^{T}\mathbf{X} - \mathbf{X_{0}}]) &= \\ 
      \arg \max_{a} (\mathrm{Cov}[a^{T}\mathbf{X}, a^{T}\mathbf{X}]) + \mathrm{Cov}[\mathbf{X_{0}}, \mathbf{X_{0}}] -2\mathrm{Cov}[a^{T}\mathbf{X}, \mathbf{X_{0}}] &= \\ 
      \arg \max_{a} (a^{T} \mathrm{Var}(\mathbf{X}) a + \mathrm{Var}(\mathbf{X_{0}}) -2a^{T} \mathrm{Cov}[\mathbf{X}, \mathbf{X_{0}}]) &= \\ 
      \arg \max_{a}(a^{T} \sigma^{2} \Sigma_{\rho} a + \sigma^{2}-2a^{T}\sigma^{2}\rho_{0})
\end{multline}

constrained by $a^{T} g(s)^{T} = g(s_{0})^{T}$ where $\Sigma_{\rho} = \frac{\mathrm{Var}(\mathbf{X})}{\sigma}^{2}$ and $\rho_{0} = \mathrm{corr(\mathbf{X}, \mathbf{X_{0}})}$ .




## c)
```{r 1b1}
# Convert the input data to a data frame
data_frame <- as.data.frame(data)

# Convert the data frame to a geodata object
temp <- as.geodata(data_frame)

# Create a regular grid of points to predict at
grid <- expand.grid(1:315,1:315)

# Use krige.conv to perform universal kriging and obtain predicted values
# krige.control is used to specify the kriging parameters
ordkrigPred <- krige.conv(temp, locations = grid, krige = 
                         krige.control(type.krige = "ok", 
                                       cov.pars = c(2500, 100), cov.model = "powered.exponential", kappa=1.5))

# Extract the predicted values from the krige object
prediction_ok <- ordkrigPred$predict

# Display the predicted values as an image plot
image.plot(interp(grid$Var1,grid$Var2,prediction_ok))

```
## d)
```{r 1b1}
# Specify expectation function and covariance model
univkrigPred <- krige.conv(temp, locations = grid, krige = 
                         krige.control(type.krige = "ok", 
                                       trend.d = "2nd", trend.l = "2nd",
                                       cov.model = "powered.exponential", 
                                       cov.pars = c(2500, 100), kappa = 1.5))

# Calculate universal Kriging predictor and associated prediction variance
prediction_uk <- univkrigPred$predict
variance <- univkrigPred$krige.var

# Display results
image.plot(interp(grid$Var1, grid$Var2, prediction_uk))
```

## e)
```{r 1b1}
# Find the index of the location s0 in the grid
s0 <- c(100, 100)
index <- which(grid$Var1 == s0[1] & grid$Var2 == s0[2])

# Calculate the probability for the elevation to be higher than 850 m
prob_higher <- 1 - pnorm(850, mean = prediction_ok[index], sd = sqrt(ordkrigPred$krige.var[index]))
cat("Probability that the elevation is higher than 850 m:", prob_higher, "\n")

# Calculate the elevation for which it is 0.90 probability that the true elevation is below it
quantile <- qnorm(0.9, mean = prediction_ok[index], sd = sqrt(ordkrigPred$krige.var[index]))
cat("Elevation for which it is 0.90 probability that the true elevation is below it:", quantile, "\n")


```
>>>>>>> OKS
## f)


# Problem 3

## a)
<<<<<<< HEAD

## b)

## c)

## d)

## e)
=======
```{r 1b1}
# Set the parameters
n <- 30
var <- 2
a <- 3

# Generate the grid
x <- 1:n
y <- 1:n
grid <- expand.grid(x, y)

# Compute the pairwise distances
distances <- as.matrix(dist(grid))

# Compute the covariance matrix
C <- var * exp(-distances/a)

# Draw one realization of the Gaussian Random Field
X <- mvrnorm(n=1, mu=rep(0, n^2), Sigma=C)


# Reshape X into a n x n matrix
X <- matrix(X, ncol=n)

# Melt the data into a data frame
data_df <- melt(X)


# Plot the Gaussian Random Field
ggplot(data_df, aes(Var1, Var2, fill=value)) +
  geom_tile() +
  scale_fill_gradientn(colours = terrain.colors(100)) +
  stat_contour(aes(z=value), color="black") +
  ggtitle("One realization of GRF") +
  xlab("X") +
  ylab("Y") +
  theme_classic()
```
## b)
```{r 1b1}
# define a semi-variogram function
semi_variogram <- function(var, distances, a) {
  return(var * (1 - exp(-distances/a)))
}

# Create a SpatialPointsDataFrame object
coords <- expand.grid(x, y)
values <- as.vector(X)
data_sp <- SpatialPointsDataFrame(coords, data.frame(values))

# Compute the empirical variogram
emp_variogram <- variog(coords = data_sp@coords, data = data_sp@data$values)
# Compute the theoretical variogram using the semi_variogram function
dVec = seq(0, 40, length.out = 200)
theo_variogram <- semi_variogram(var, dVec, a)

# Plot the true and empirical variograms together
plot(emp_variogram, pch = 16, col = "blue", main = "True and Empirical Semi-variograms", xlab = "Distance", ylab = "Semi-variance", ylim=c(0,3))
lines(dVec, theo_variogram, col = "red", ylim=c(0,3))
legend("bottomright", legend = c("True semi-variogram", "Empirical semi-variogram"), col = c("red", "blue"), lty = 1)

```

## c)
```{r 1b1}
#Repeat problem a) and b) three times. Making for-loop for doing it

# Define the number of repetitions
n_repetitions <- 3


# Loop through the repetitions
for (i in 1:n_repetitions) {
  # Generate the grid
  x <- 1:n
  y <- 1:n
  grid <- expand.grid(x, y)
  
  # Compute the pairwise distances
  distances <- as.matrix(dist(grid))
  
  # Compute the covariance matrix
  C <- var * exp(-distances/a)
  
  # Draw one realization of the Gaussian Random Field
  X <- mvrnorm(n=1, mu=rep(0, n^2), Sigma=C)
  
  # Reshape X into a n x n matrix
  X <- matrix(X, ncol=n)
  
  # Melt the data into a data frame
  data_df <- melt(X)
  
  # Plot the Gaussian Random Field
  plot_GRF <- ggplot(data_df, aes(Var1, Var2, fill=value)) +
    geom_tile() +
    scale_fill_gradientn(colours = terrain.colors(100)) +
    stat_contour(aes(z=value), color="black") +
    ggtitle(paste0("One realization of GRF, repetition ", i)) +
    xlab("X") +
    ylab("Y") +
    theme_classic()
  
  # Print the plot object
  print(plot_GRF)
  
  # Create a SpatialPointsDataFrame object
  coords <- expand.grid(x, y)
  values <- as.vector(X)
  data_sp <- SpatialPointsDataFrame(coords, data.frame(values))
  
  # Compute the empirical variogram
  emp_variogram <- variog(coords = data_sp@coords, data = data_sp@data$values)
  
  # Compute the theoretical variogram using the semi_variogram function
  dVec = seq(0, 40, length.out = 200)
  theo_variogram <- semi_variogram(var, dVec, a)
  
  # Plot the true and empirical variograms together
  plot(emp_variogram, pch = 16, col = "blue", main = paste0("True and Empirical Semi-variograms, repetition ", i), xlab = "Distance", ylab = "Semi-variance")
  lines(dVec, theo_variogram, col = "red")
  legend("bottomright", legend = c("True semi-variogram", "Empirical semi-variogram"), col = c("red", "blue"), lty = 1)
}
```
## d)
```{r 1b1}
set.seed(123) 
# Select 36 locations uniformly at random
sampled_points <- sample.int(n^2, 36, replace = FALSE)
x_sampled <- grid[sampled_points,1]
y_sampled <- grid[sampled_points,2]
sampled_coords <- cbind(x_sampled, y_sampled)

# Get the exact observations at the sampled locations
exact_obs <- X[sampled_points]

# Create a SpatialPointsDataFrame object
data_sp <- SpatialPointsDataFrame(sampled_coords, data.frame(exact_obs))
data_sp@data$values <- data_sp$exact_obs


# Compute the empirical variogram
emp_variogram <- variog(coords = data_sp@coords, data = data_sp@data$values)

# Compute the theoretical variogram using the semi_variogram function
dVec = seq(0, 40, length.out = 200)
theo_variogram <- semi_variogram(var,dVec , a)

# Plot the true and empirical variograms together
plot(emp_variogram, pch = 16, col = "blue", main = "True and Empirical Semi-variograms", xlab = "Distance", ylab = "Semi-variance", ylim =c(0,3))
lines(dVec, theo_variogram, col = "red")
legend("topright", legend = c("True semi-variogram", "Empirical semi-variogram"), col = c("red", "blue"), lty = 1)

##
initial_values = c(1, 1)
fit_full <- likfit(coords = coords, data = X, ini.cov.pars = initial_values, cov.model = "exponential", fix.nugget = TRUE)
fit_obs = likfit(coords=as.matrix(data_sp@coords), data = data_sp@data$values ,
                 ini.cov.pars=initial_values, cov.model="exponential",
                 fix.nugget=TRUE)
print(fit_full$cov.pars)
print(fit_obs$cov.pars)

est_variogram_full <- semi_variogram(fit_full$cov.pars[1], dVec, fit_full$cov.pars[2])
est_variogram_obs <- semi_variogram(fit_obs$cov.pars[1], dVec, fit_obs$cov.pars[2])

#plot estimated and true semi-variogram for all locations
plot(est_variogram_full, pch = 16, col = "blue", main = "True and Estimated Semi-variograms for all locations on D", xlab = "Distance", ylab = "Semi-variance", ylim=c(0, 3), xlim = c(0,40))
lines(dVec, theo_variogram, col = "red")
legend("topright", legend = c("True semi-variogram", "Estimated semi-variogram"), col = c("red", "blue"), lty = 1)

#plot estimated and true semi-variogram for the 36 locations
plot(est_variogram_obs, pch = 16, col = "blue", main = "True and Estimated Semi-variograms for 36 locations", xlab = "Distance", ylab = "Semi-variance", ylim=c(0, 3), xlim = c(0,40))
lines(dVec, theo_variogram, col = "red")
legend("topright", legend = c("True semi-variogram", "Estimated semi-variogram"), col = c("red", "blue"), lty = 1)


```


## e)
```{r 1b1}
#Starting with case where we have 9 locations
set.seed(123) 
# Select 9 locations uniformly at random
sampled_points9 <- sample.int(n^2, 9, replace = FALSE)
x_sampled9 <- grid[sampled_points9,1]
y_sampled9 <- grid[sampled_points9,2]
sampled_coords9 <- cbind(x_sampled9, y_sampled9)
# Get the exact observations at the sampled locations
exact_obs9 <- X[sampled_points9]

# Create a SpatialPointsDataFrame object for 9 locations
data_sp9 <- SpatialPointsDataFrame(sampled_coords9, data.frame(exact_obs9))
data_sp9@data$values <- data_sp9@data$exact_obs9


#Creating 64 random locations
set.seed(123) 
# Select 64 locations uniformly at random
sampled_points64 <- sample.int(n^2, 64, replace = FALSE)
x_sampled64 <- grid[sampled_points64,1]
y_sampled64 <- grid[sampled_points64,2]
sampled_coords64 <- cbind(x_sampled64, y_sampled64)
# Get the exact observations at the sampled locations
exact_obs64 <- X[sampled_points64]

#Creating 100 random locations
set.seed(123) 
# Select 64 locations uniformly at random
sampled_points100 <- sample.int(n^2, 100, replace = FALSE)
x_sampled100 <- grid[sampled_points100,1]
y_sampled100 <- grid[sampled_points100,2]
sampled_coords100 <- cbind(x_sampled100, y_sampled100)
# Get the exact observations at the sampled locations
exact_obs100 <- X[sampled_points100]


# Create a SpatialPointsDataFrame object for 9 locations
data_sp9 <- SpatialPointsDataFrame(sampled_coords9, data.frame(exact_obs9))
data_sp9@data$values <- data_sp9$exact_obs9

# Create a SpatialPointsDataFrame object for 64 locations
data_sp64 <- SpatialPointsDataFrame(sampled_coords64, data.frame(exact_obs64))
data_sp64@data$values <- exact_obs64

# Create a SpatialPointsDataFrame object for 100 locations
data_sp100 <- SpatialPointsDataFrame(sampled_coords100, data.frame(exact_obs100))
data_sp100@data$values <- exact_obs100

initial_values = c(1, 1)
fit_obs9 = likfit(coords=as.matrix(data_sp9@coords), data = data_sp9@data$values ,
                 ini.cov.pars=initial_values, cov.model="exponential",
                 fix.nugget=TRUE)
fit_obs64 = likfit(coords=as.matrix(data_sp64@coords), data = data_sp64@data$values ,
                  ini.cov.pars=initial_values, cov.model="exponential",
                  fix.nugget=TRUE)
fit_obs100 = likfit(coords=as.matrix(data_sp100@coords), data = data_sp100@data$values ,
                   ini.cov.pars=initial_values, cov.model="exponential",
                   fix.nugget=TRUE)

print(fit_obs9$cov.pars)
print(fit_obs64$cov.pars)
print(fit_obs100$cov.pars)

#making the estimated semi-variograms for the cases with 9, 64 and 100 locations
est_variogram_obs9 <- semi_variogram(fit_obs9$cov.pars[1], dVec, fit_obs9$cov.pars[2])
est_variogram_obs64 <- semi_variogram(fit_obs64$cov.pars[1], dVec, fit_obs64$cov.pars[2])
est_variogram_obs100 <- semi_variogram(fit_obs100$cov.pars[1], dVec, fit_obs100$cov.pars[2])

#plot estimated and true semi-variogram for the 9 locations
plot(est_variogram_obs9, pch = 16, col = "blue", main = "True and Estimated Semi-variograms for 9 locations", xlab = "Distance", ylab = "Semi-variance", ylim=c(0, 3), xlim = c(0,40))
lines(dVec, theo_variogram, col = "red")
legend("topright", legend = c("True semi-variogram", "Estimated semi-variogram"), col = c("red", "blue"), lty = 1)

#plot estimated and true semi-variogram for the 64 locations
plot(est_variogram_obs64, pch = 16, col = "blue", main = "True and Estimated Semi-variograms for 64 locations", xlab = "Distance", ylab = "Semi-variance", ylim=c(0, 3), xlim = c(0,40))
lines(dVec, theo_variogram, col = "red")
legend("topright", legend = c("True semi-variogram", "Estimated semi-variogram"), col = c("red", "blue"), lty = 1)

#plot estimated and true semi-variogram for the 100 locations
plot(est_variogram_obs100, pch = 16, col = "blue", main = "True and Estimated Semi-variograms for 100 locations", xlab = "Distance", ylab = "Semi-variance", ylim=c(0, 3), xlim = c(0,40))
lines(dVec, theo_variogram, col = "red")
legend("topright", legend = c("True semi-variogram", "Estimated semi-variogram"), col = c("red", "blue"), lty = 1)





```
>>>>>>> OKS

## f)