---
title: "TMA4250-Project-1"
author: "Ole Riddervold, Ole Kristian Skogly"
date: "2023-02-07"
output: html_document
---
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)

# install.packages("geoR")
# install.packages("akima")
# install.packages("fields")
# install.packages("ggplot2")
# install.packages("MASS")
# install.packages("reshape2")
# install.packages("patchwork")
# install.packages("glue")
# install.packages("tidyverse")
# install.packages("latex2exp")

library(geoR)
library(akima)
library(fields)
library(ggplot2)
library(MASS)
library(reshape2)
library(patchwork)
library(glue)
library(tidyverse)
library(latex2exp)
```

# Problem 1

## a)

## b)
$X$ is a GRF, and therefore, by definition,
$$
\mathbf{X} \sim \mathcal{N}(\mathbf{\mu}, \Sigma)
$$
where $\mu_i = m(s_i)$ and $\Sigma_{ij} = c(s_i, s_j)$. In this particular case, we get $\mathbf{\mu} = \mathbf{0}$ and $\Sigma_{ij} = \sigma^2 \rho(|s_i-s_j|)$, where $\rho$ is the *correlation function*, in this case of type either powered exponential or matern.

The following code yields a simulation with four realizations for the values $\sigma^2 \in \{1, 5\}$, $\alpha \in \{1, 1.9\}$, $\nu \in \{1, 3\}$:
```{r 1b0}
powered_exponential <- function(h, a, alpha) {
  # Powered exponential correlation function as defined in lectures
  return(
    exp(-(h/a)^alpha)
  )
}

matern_modified <- function(h, a, nu) {
  # Matern correlation function as defined in lectures (based on matern from geoR)
  phi <- a/sqrt(8*nu)
  return(
    matern(h, phi, nu)
  )
}
```

Defining parameters that are constant for the different cases:
```{r 1b1}
s <- c(1:50)
N <- 4
variance <- c(1, 5)
array_dims <- c(4, 4, length(s)) # Dimensions of the data-array to be plotted

dist_matrix <- as.matrix(dist(s, diag=TRUE, upper=TRUE))
mu <- rep(0, length(s))
```

Defining a function to plot the resulting 3D-array:
```{r 1b2_plot_fnc}
plot_realizations <- function(realizations) {
  
  plot <- do.call(wrap_plots, lapply(1:4, function(i) { # Using patchwork to create subplots using `wrap_plots`
    df <- realizations %>% melt %>% filter(Var1 == i)   # Converts the 3D array into a dataframe and filters on i
    ggplot(df) + geom_line(aes(Var3, value, group=Var2, color=factor(Var2))) +
      xlab("s") + labs(title=glue("Case {i}"), color="")
  }))
  
  return(plot)
}
```


Loop for powered exponential correlation function:
```{r 1b2_sim}
alpha <- c(1, 1.9)
exp_a <- 10
exp_realizations <- array(rep(0, prod(array_dims)), array_dims)

for (i in seq_along(variance)) {
  for (j in seq_along(alpha)) {
    Sigma <- variance[i]*powered_exponential(dist_matrix, exp_a, alpha[j])
    flattened_index <- 2*(i-1)%%2 + j
    exp_realizations[flattened_index, , ] <- as.array(mvrnorm(n=N, mu, Sigma))
  }
}
```

```{r 1b2_plot}
plot_realizations(exp_realizations)
```


Loop for matern correlation functions:
```{r 1b3_sim}
nu_values <- c(1, 3)
matern_a <- 10
matern_realizations <- array(rep(0, prod(array_dims)), array_dims)

for (i in seq_along(variance)) {
  for (j in seq_along(nu_values)) {
    Sigma <- variance[i]*matern(dist_matrix, matern_a, nu_values[j])
    flattened_index <- 2*(i-1)%%2 + j
    matern_realizations[flattened_index, , ] <- as.array(mvrnorm(n=N, mu, Sigma))
  }
}
```

```{r 1b3_plot}
plot_realizations(matern_realizations)
```

## c)

We have $\mathbf{Y} = \mathbf{X}^* + \mathbf{\varepsilon}$, with $\mathbf{X}^* = \left(X(s_1), \: X(s_2), \: X(s_3)\right)^T$, $\mathbf{X}^*$ is distributed as above, but only for the first three entries of $\tilde{\mathcal{D}}$, and $\mathbf{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2_NI)$. Since $\mathbf{X}^*$ and $\mathbf{\varepsilon}$ are uncorrelated, we get
$$
\mathbf{Y} \sim \mathcal{N}(\mathbf{0}, \bar{\Sigma} + \sigma^2_NI)
$$
where $\bar{\Sigma} = \mathrm{Var}(\mathbf{X}^*)$.

## d)

In this problem, consider
$$
\begin{pmatrix}
  \mathbf{X} \\
  \mathbf{Y}
\end{pmatrix}
\sim
\mathcal{N}\left( \mathbf{0},
  \begin{pmatrix}
    \Sigma_{00} & \Sigma_{01} \\
    \Sigma_{10} & \Sigma_{11}
  \end{pmatrix}
\right)
$$
where $\Sigma_{00} = \Sigma$, $\Sigma_{11} = \bar{\Sigma} + \sigma_N^2I$, and
$$
\begin{align}
  \Sigma_{10} = \Sigma_{01}^T &= \mathrm{Cov}(\mathbf{X}, \mathbf{Y}) = \mathrm{Cov}(\mathbf{X}, \mathbf{X}^*+\mathbf{\varepsilon}) \\
  &= \Sigma^* + \mathrm{Cov}(\mathbf{X}, \mathbf{\varepsilon}) = \Sigma^* \equiv \mathrm{Cov}(\mathbf{X}^*, \mathbf{X})
\end{align}
$$

Thus, looking at a formula for basic conditional expectation for multivariate gaussian distributions, we get:
$$
\mathbf{X} | \mathbf{Y}=\mathbf{y} \sim \mathcal{N}\left( \Sigma^{*T}\left(\bar{\Sigma}+\sigma_N^2I\right)^{-1}\mathbf{y}, \Sigma - \Sigma^{*T}\left(\bar{\Sigma}+\sigma_N^2I\right)^{-1} \Sigma^* \right)
$$
Simulation in R:

```{r 1d1}
Sigma <- variance[2]*matern(dist_matrix, matern_a, nu_values[2])
sigma_N <- c(0, 0.25)
Sigma_star <- Sigma[1:3, ]
Sigma_bar <- Sigma_star[, 1:3]

array_dims <- c(50, 4)
cond_sim <- array(rep(0, prod(array_dims)), array_dims)
for (i in seq_along(sigma_N)) {
  y <- as.vector(mvrnorm(n=1, rep(0, 3), Sigma_bar + sigma_N[i]*identity(3)))
  inv_mat <- solve(Sigma_bar + sigma_N[i]*identity(3))
  cond_mu <-  t(Sigma_star) %*% inv_mat %*% y
  cond_Sigma <- Sigma - t(Sigma_star) %*% inv_mat %*% Sigma_star
  cond_sim[, i] <- mvrnorm(n=1, cond_mu, cond_Sigma)
  cond_sim[, i+2] <- diag(cond_Sigma)
}
```

Creating a dataframe:
```{r 1d2}
z <- 1.64 # z-value for a confidence interval of level 0.10

cond_sim <- as.data.frame(cond_sim)
colnames(cond_sim) <- c("Rel1", "Rel2", "Rel1_95", "Rel2_95")
cond_sim$s <- s
cond_sim$Rel1_95 <- sqrt(cond_sim$Rel1_95)*z
cond_sim$Rel2_95 <- sqrt(cond_sim$Rel2_95)*z
head(cond_sim)
```


Plotting:
```{r 1d3}
p1 <- ggplot(cond_sim) +
  geom_line(aes(s, Rel1)) +
  geom_ribbon(aes(x=s, ymin=Rel1-Rel1_95, ymax=Rel1+Rel1_95), alpha=0.25) +
  ylab("Realization 1") + labs(title=TeX(glue("$\\sigma_N^2 = {sigma_N[1]}$")))

p2 <- ggplot(cond_sim) +
  geom_line(aes(s, Rel2)) + 
  geom_ribbon(aes(x=s, ymin=Rel2-Rel2_95, ymax=Rel2+Rel2_95), alpha=0.25) +
  ylab("Realization 2") + labs(title=TeX(glue("$\\sigma_N^2 = {sigma_N[2]}$")))

p1 + p2
```
Questions: How do I add a legend to this plot? How do I make the plots wider?

## e)

Simulation:
```{r 1e1}
estimate_cond_realization <- function(sigma_N, n) {
  # Given some nugget variance, simulates conditional distribution of X
  # on the given grid with observation y = (y_1, y_2, y_3).
  y <- as.vector(mvrnorm(n=1, rep(0, 3), Sigma_bar + sigma_N*identity(3)))
  inv_mat <- solve(Sigma_bar + sigma_N*identity(3))
  cond_mu <-  t(Sigma_star) %*% inv_mat %*% y
  cond_Sigma <- Sigma - t(Sigma_star) %*% inv_mat %*% Sigma_star
  return(mvrnorm(n=n, cond_mu, cond_Sigma))
}

array_dims <- c(2, 50, 100)
cond_sim <- array(rep(0, prod(array_dims)), array_dims)
for (i in seq_along(sigma_N)) {
  cond_sim[i, ,] <- estimate_cond_realization(sigma_N[i], n=100)
}
```
Note: This also holds for any other choice of locations, but using the first three makes indexing easier.

Estimation:
```{r 1e2}
melted_cond_sim <- cond_sim %>%
  melt() %>%
  group_by(Var1, Var2) %>%
  mutate(mean_est=mean(value), CI90_est=1.64*sqrt(1+var(value))/10) %>%
  nest(-Var1)

head(melted_cond_sim$data[1])
```
```{r 1e3}
p1 <- as.data.frame(melted_cond_sim$data[1]) %>% ggplot() +
  geom_line(aes(x=Var2, y=value, group=Var3, color=factor(Var3))) +
  geom_line(aes(x=Var2, y=mean_est)) +
  geom_ribbon(aes(x=Var2, ymin=mean_est-CI90_est, ymax=mean_est+CI90_est), alpha=0.25) +
  xlab("s") + ylab("Relizations") +
  labs(title=TeX(glue("$\\sigma_N^2 = {sigma_N[1]}$"))) +
  theme(legend.position="none")

p2 <- as.data.frame(melted_cond_sim$data[2]) %>% ggplot() +
  geom_line(aes(x=Var2, y=value, group=Var3, color=factor(Var3))) +
  geom_line(aes(x=Var2, y=mean_est)) +
  geom_ribbon(aes(x=Var2, ymin=mean_est-CI90_est, ymax=mean_est+CI90_est), alpha=0.25) +
  xlab("s") + ylab("Relizations") +
  labs(title=TeX(glue("$\\sigma_N^2 = {sigma_N[2]}$"))) +
  theme(legend.position="none")

p1 + p2
```


## f)
Regular MC integral estimate of $\mathbb{E}(A)$:
```{r 1f1}
sim <- cond_sim[1, , ]
sim_stats <- sim %>%
  melt() %>%
  group_by(Var2) %>%
  summarise(A_hat=sum((value>2)*(value-2))) %>%
  summarise(A_bar=mean(A_hat), sigma_hat=sd(A_hat))

sim_stats
```
One can use simple kriging to calculate predictions at all values of $s$ based on our observation $\mathbf{y}$. The formula is as follows:
$$
\begin{align}
  \hat{\mathbf{X}} &= \mathbf{a}^T\mathbf{Y} \\
  \mathbf{a} &= \Sigma_{\mathbf{Y}}\mathrm{Cov}(\mathbf{Y}, \mathbf{X}) 
\end{align}
$$
where both $\Sigma_{\mathbf{Y}}\in \mathbb{R}^{3\times3}$ and $\mathrm{Cov}(\mathbf{Y}, \mathbf{X}) \in \mathbb{R}^{50\times 3}$ can be retrieved from $\Sigma$.

Simple 1D Kriging using the matern covariance function:
```{r 1f2, seed=TRUE}
y <- as.vector(mvrnorm(n=1, rep(0, 3), Sigma_bar)) # Draw of observed y with sigma_N = 0

c_matrix <- Sigma[, 1:3] # Represents Cov(Y, X_0) for all X_0 in D
a_matrix <- c_matrix %*% t(solve(Sigma_bar)) # Calculation of all a for the BLUP
X_hat_kriging <- a_matrix %*% y
```

```{r 1f3}
krig_stats <- X_hat_kriging %>%
  melt()

krig_stats
```

Firstly, $\hat{X}(s)$ is an unbiased predictor, so $\mathbb{E}(\hat{X}(s))=X(s)$. Furthermore, Jensen's inequality states that for a concave function $g: \mathbb{R} \rightarrow \mathbb{R}$
$$
g\left(\mathbb{E}(X)\right) \leq \mathbb{E}\left(g(X)\right)
$$
In this case, $g: \mathcal{L}^2 \rightarrow \mathcal{L}^2$ is given by
$$
g(x) = \sum_{s\in\tilde{\mathcal{D}}}{\mathbb{I}(x(s)>2)(x(s)-2)}
$$
where $\mathcal{L}^2$ is the hilbert space of random variables (in this case a random field indexed by $s$). $g$ is convex, but not strictly convex, i.e.
$$
\begin{align}
  \tilde{A}=g\left(\mathbb{E}(X)\right) &\leq \mathbb{E}\left(g(X)\right)=\hat{A} \\
  \implies \tilde{A} &\leq \hat{A}
\end{align}
$$

## g)


# Problem 2

## a)

## b)

## c)

## d)

## e)

## f)


# Problem 3

## a)

## b)

## c)

## d)

## e)

## f)